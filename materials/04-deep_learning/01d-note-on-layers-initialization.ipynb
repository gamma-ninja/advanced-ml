{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Note on the network initialization\n",
    "\n",
    "    Thomas Moreau <thomas.moreau@inria.fr>\n",
    "    Alexandre Gramfort <alexandre.gramfort@inria.fr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we consider the properties of different initialization schemes for the parameter of the network. We will some random data `x, y` generated bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_samples, n_features = 1000, 100\n",
    "\n",
    "x = torch.randn(n_samples, n_features)\n",
    "y = torch.randn(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider one linear layer that conserves the input dimensionality.  \n",
    "We initialize the weights and the bias randomly with normal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "\n",
    "def lin(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "w1 = torch.randn(n_features, n_hidden)\n",
    "b1 = torch.randn(n_hidden)\n",
    "l1 = lin(x, w1, b1)\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with the way our model was initialized, however.  \n",
    "To understand it, we need to look at the standard deviation (std) of `l1` compared to the one of the input `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.std(), l1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation, which represents how far away our activations go from the mean, went from 1 to 10.  \n",
    "\n",
    "This is a really big problem because that's with just one layer. Modern neural nets can have hundreds of layers, so if each of them multiplies the scale of our activations by 10, by the end of the last layer we won't have numbers representable by a computer.\n",
    "\n",
    "Indeed, if we apply just 50 layers with such initialization `x`, we'll have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n_samples, n_features)\n",
    "for i in range(50):\n",
    "    w1 = torch.randn(n_features, n_hidden)\n",
    "    b1 = torch.randn(n_hidden)\n",
    "    x = lin(x, w1, b1)\n",
    "    \n",
    "print(f\"std(X) = {x.std().item():.2e}\")\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is `nan`s everywhere. So maybe the scale of our matrix was too big, and we need to have smaller weights? But if we use too small weights, we will have the opposite problemâ€”the scale of our activations will go from 1 to 0.1, and after 50 layers we'll be left with zeros everywhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n_samples, n_features)\n",
    "for i in range(50):\n",
    "    w1 = 1e-2 * torch.randn(n_features, n_hidden)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    x = lin(x, w1, b1)\n",
    "    \n",
    "print(f\"std(X) = {x.std().item():.2e}\")\n",
    "x[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put `b1` to `0` here as otherwise the output is not 0 everywhere, but the same for each input.\n",
    "\n",
    "The quantity that drives this phenomena is the operator norm of the weight matrix $\\|W_1\\|_2$, which corresponds to its largest eigenvector. So if we want to control this, we have to scale our weights to have a norm close to $1$ so it does not explode to $\\infty$ or $0$. Using random matrices theory, this can easily be done, as illustrated by Xavier Glorot and Yoshua Bengio in [\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\"](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). The right scale for a given layer is $1/\\sqrt{n_{in}}$, where $n_{in}$ represents the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = torch.randn(n_samples, n_features)\n",
    "for i in range(50):\n",
    "    w1 = torch.randn(n_features, n_hidden) / np.sqrt(n_features)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    x = lin(x, w1, b1)\n",
    "\n",
    "print(f\"std(X) = {x.std().item()}\")\n",
    "x[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally some numbers that are neither zeros nor `nan`s and a reasonable std!\n",
    "\n",
    "If you play a little bit with the value for scale by moving `eps` to $\\{-1, 1\\}$, you'll notice that even a slight variation from $\\frac1{n_{in}}$ will get you either to very small or very large numbers, so initializing the weights properly is extremely important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = -1\n",
    "x = torch.randn(n_samples, n_features)\n",
    "for i in range(50):\n",
    "    w1 = torch.randn(n_features, n_hidden) / (np.sqrt(n_features) + eps)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    x = lin(x, w1, b1)\n",
    "\n",
    "print(f\"std(X) = {x.std().item():.2f}\")\n",
    "x[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good. Now we need to go through a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp_min(0.)\n",
    "\n",
    "\n",
    "# Redefine fresh `x, y`\n",
    "x = torch.randn(n_samples, n_features)\n",
    "y = torch.randn(n_samples)\n",
    "\n",
    "\n",
    "w1 = torch.randn(n_features, n_hidden) / np.sqrt(n_features)\n",
    "b1 = torch.randn(n_hidden)\n",
    "l1 = lin(x, w1, b1)\n",
    "l2 = relu(l1)\n",
    "l2.mean(), l2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're back to square one: the mean of our activations has gone to up (which is understandable since we removed the negatives) and the std went down. So like before, after a few layers we will probably end up with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n_samples, n_features)\n",
    "\n",
    "for i in range(50):\n",
    "    w1 = torch.randn(n_features, n_hidden) / np.sqrt(n_features, dtype=np.float32)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    x = relu(lin(x, w1, b1))\n",
    "\n",
    "print(f\"std(X) = {x.std().item():.2e}\")\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our initialization wasn't right. Why? At the time Glorot and Bengio wrote their article, the popular activation in a neural net was the hyperbolic tangent (tanh, which is the one they used), and that initialization doesn't account for our ReLU. Fortunately, someone else has done the math for us and computed the right scale for us to use. In [\"Delving Deep into Rectifiers: Surpassing Human-Level Performance\"](https://arxiv.org/abs/1502.01852) (it's the article that introduced the ResNet), Kaiming He et al. show that we should use the following scale instead: $\\sqrt{2 / n_{in}}$, where $n_{in}$ is the number of inputs of our model. Let's see what this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n_samples, n_features)\n",
    "\n",
    "for i in range(50):\n",
    "    w1 = torch.randn(n_features, n_hidden) * np.sqrt(2 / n_features, dtype=np.float32)\n",
    "    b1 = torch.zeros(n_hidden)\n",
    "    x = relu(lin(x, w1, b1))\n",
    "\n",
    "print(f\"std(X) = {x.std().item():.2f}\")\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better: our numbers aren't all zeroed this time.  \n",
    "This initialization is named *Kaiming initialization* or *He initialization*.\n",
    "\n",
    "Note that these consideration are linked to the Lipschitz constants of the layers and of the network, that have received significant attention in the recent literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things to remember:\n",
    "\n",
    "- A neural net is basically a bunch of matrix multiplications with nonlinearities in between.\n",
    "- When subclassing `nn.Module`, we have to call the superclass `__init__` method in our `__init__` method and we have to define a `forward` function that takes an input and returns the desired result.\n",
    "- The backward pass is the chain rule applied multiple times, computing the gradients from the output of our model and going back, one layer at a time.\n",
    "- Properly initializing a neural net is crucial to get training started. Kaiming initialization should be used when we have ReLU nonlinearities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
