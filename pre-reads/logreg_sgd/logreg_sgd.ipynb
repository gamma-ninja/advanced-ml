{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Logistic Regression\n",
    "\n",
    "    Thomas Moreau <thomas.moreau@inria.fr>\n",
    "    Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "    \n",
    "    Notebook inspired from materials by M. Le Morvan, O. Grisel.\n",
    "\n",
    "In this notebook, we  will implement a logistic regression model in Python using only `numpy` library (and `matplotlib` for visualization), to get the basic tools necessary to implement a neural network for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "\n",
    "1. [Data Generation](#data)  \n",
    "2. [The Logistic Regression](#logreg)  \n",
    "    2.1. [Loss and Gradient](#2.1---Loss-and-Gradient)  \n",
    "    2.2. [Stochastic Gradient Descent](#2.2---Stochastic-Gradient-Descent)  \n",
    "    2.3. [Scikit-learn Model](#2.3---Scikit-learn-Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Data Generation\n",
    "\n",
    "<a id='data'></a>\n",
    "\n",
    "To test our model on very simple example, we will use Gaussian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_data\n",
    "from utils import plot_data, show_decision_boundary\n",
    "\n",
    "MU1 = (2, 0)\n",
    "MU2 = (-1, np.sqrt(3))\n",
    "MU3 = (-1, -np.sqrt(3))\n",
    "\n",
    "l_mu = [MU1, MU2, MU3]\n",
    "X, y = generate_data(l_mu, random_state=42)\n",
    "\n",
    "# One hot encoded target\n",
    "y_ohe = OneHotEncoder().fit_transform(y[:, None]).toarray()\n",
    "\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into a training and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_val,\n",
    " y_train, y_val,\n",
    " y_ohe_train, y_ohe_val) = train_test_split(X, y, y_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Logistic Regression\n",
    "\n",
    "<a id='logreg'></a>\n",
    "\n",
    "\n",
    "### 2.1 - The model\n",
    "\n",
    "We considers $N$ data points $(\\mathbf{x}_1, \\dots, \\mathbf{x}_N)$ in $\\mathbb{R}^{D}$ belonging to $C$ possible classes.\n",
    "\n",
    "We want to solve the classification task for these points, _i.e.,_ learn the parameters $\\theta = (\\mathbf{w}, \\mathbf{b}) \\in \\mathbb{R}^{C\\times D}\\times \\mathbb{R}^{C}$ of the function $f_\\theta: \\mathbb{R}^D \\to [0, 1]^C$ which corresponds for each coordinate to the probability of being from one class. For a given $\\mathbf{x}$, the model is defined as\n",
    "$$\n",
    "\\hat{\\mathbb{P}}[Y=c |\\mathbf{X} = \\mathbf{x}] = \\frac{1}{Z} \\exp(\\mathbf{w}_c^\\top\\mathbf{x}+b_c).\n",
    "$$\n",
    "As these probabilities must sum to one, we get\n",
    "$$\n",
    "Z = \\sum_{c=1}^C \\exp(\\mathbf{w}_c^\\top\\mathbf{x}+b_c).\n",
    "$$\n",
    "We can recognize the so-called _soft-max_ function $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice:</b>\n",
    "\n",
    "* Write the function that returns the probability of being of one class given the parameters `w, b` of the model and the input `X`.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Hint:** You can use the function `scipy.special.softmax`\n",
    "\n",
    "Solution in `solutions/0_logreg_model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "def predict_proba_logreg(w, b, X):\n",
    "    \"\"\"Return the proba of being in one of two classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_classes, n_features)\n",
    "        parameters for the linear model.\n",
    "    b : ndarray, shape (n_classes,)\n",
    "        biases for the linear model.\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        input data\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    y_proba : ndarray, shape (n_samples, n_classes)\n",
    "        probability of being from each class according to\n",
    "        the linear model w.\n",
    "    \"\"\"\n",
    "\n",
    "    ####################\n",
    "    # TO DO\n",
    "\n",
    "    # END TO DO\n",
    "    ####################\n",
    "    return y_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the decision boundary of such model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[3, 2], [-2, 3], [5, 0]])\n",
    "b = np.array([.1, -.1, .3])\n",
    "\n",
    "y_pred = predict_proba_logreg(w, b, X_val).argmax(axis=1)\n",
    "print(f\"Model accuracy: {np.mean(y_pred == y_val)}\")\n",
    "\n",
    "show_decision_boundary(partial(predict_proba_logreg, w=w, b=b), data=(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss and Gradient\n",
    "\n",
    "To learn the model parameters $\\mathbf{w}_c, b_c$ on a training set $\\{\\mathbf{x}_i, y_i\\}$, we minimize the negative log likelihood (**nll**, _a.k.a_ cross-enropy loss):\n",
    "$$\n",
    "L(\\mathbf{w}, \\mathbf{b}) = - \\frac1N\\sum_{i=1}^N \\log(\\hat{\\mathbb{P}}[Y=y_i |\\mathbf{X} = \\mathbf{x}_i]) = -\\frac1N \\sum_{i=1}^N \\log\\Bigg(\\frac{\\mathbf{w}_{y_i}^\\top\\mathbf{x}_i+b_{y_i}}{\\sum_{c=1}^C \\exp(\\mathbf{w}_{c}^\\top\\mathbf{x}_i+b_{c})}\\Bigg)\n",
    "$$\n",
    "\n",
    "\n",
    "As we saw previously, to train a model, we will need to compute the gradients of this loss with respect to its parameters. Let's first start with the derivative of the loss relative to the prediction for one sample. If we denote $Y$ the one-hot encoded target for one sample and $\\mathbf{\\hat y}$ corresponds to the output of the model, the **nll** reads:\n",
    "$$\n",
    "\\ell(Y, \\mathbf{\\hat y}) = - \\sum_{c=1}^C Y_{c} \\log(\\mathbf{\\hat y}_{c})\n",
    "= - \\sum_{c=1}^C Y_c \\left(z_c - \\log\\left(\\sum_{l=1}^C \\exp(z_l)\\right)\\right).\n",
    "$$\n",
    "where $z_c = \\mathbf{w}_{c}^\\top\\mathbf{x}_i+b_{c}$\n",
    "\n",
    "Consequently,\n",
    "$$\n",
    "\\frac{\\partial \\ell }{\\partial z_j} = - \\left(Y_j - \\sum_{c=1}^C Y_c \\frac{\\exp(z_j)}{\\sum_{l=1}^C \\exp(z_l)} \\right) = -\\left(Y_j - \\sum_{c=1}^C Y_c \\mathbf{ \\hat y}_j \\right) = \\mathbf{ \\hat y}_j - Y_j.\n",
    "$$\n",
    "\n",
    "Note that deriving the log of the softmax makes calculations much easier.  \n",
    "Now, using the chain rule, we can easily compute the gradient for $\\mathbf{w}$ and $\\mathbf{b}$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\partial \\ell}{\\partial w_j} = \\frac{\\partial z_j}{\\partial w_j}^\\top\\frac{\\partial \\ell}{\\partial z_j} = (\\mathbf{\\hat y}_j - Y_j)\\mathbf{x}\\\\\n",
    "    \\frac{\\partial \\ell}{\\partial b_j} = \\frac{\\partial z_j}{\\partial b_j}^\\top\\frac{\\partial \\ell}{\\partial z_j} = (\\mathbf{\\hat y}_j - Y_j)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "And using the linearity of the gradient, we get:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\partial L}{\\partial w_j} = \\frac1N\\sum_{i=1}^N(\\mathbf{\\hat y}_{ij} - Y_{ij})\\mathbf{x_i}\\\\\n",
    "    \\frac{\\partial L}{\\partial b_j} = \\frac1N\\sum_{i=1}^N (\\mathbf{\\hat y}_{ij} - Y_{ij})\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice:</b>\n",
    "\n",
    "* Write the corresponding function which returns the log-likelihood of the model as well as its gradient.\n",
    "\n",
    "</div>\n",
    "\n",
    "Solution in `solutions/1_logreg_gradient.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_and_grad(w, b, X, y):\n",
    "    \"\"\"Log-likelihood of the logistic regression model and gradient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_classes, n_features)\n",
    "        parameters for the linear model.\n",
    "    b : ndarray, shape (n_classes,)\n",
    "        biases for the linear model.\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        input data\n",
    "    y : ndarray, shape (n_samples, n_classes)\n",
    "        output targets one hot encoded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : log-likelihood of the logreg model\n",
    "    grad_w : gradient of the model parameters w.\n",
    "    grad_b : gradient of the model parameters b.\n",
    "    \"\"\"\n",
    "\n",
    "    #####################\n",
    "    # TO DO\n",
    "\n",
    "    # END TO DO\n",
    "    #####################\n",
    "    return loss, grad_w, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[3, 2], [-2, 3], [5, 0]])\n",
    "b = np.array([.1, -.1, .3])\n",
    "\n",
    "loss, grad_w, grad_b = log_likelihood_and_grad(w, b, X_train, y_ohe_train)\n",
    "print(\"Computed loss =\", loss)\n",
    "assert np.allclose(loss, 3.004595)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Stochastic Gradient Descent\n",
    "\n",
    "The stochastic gradient descent for this model consists in taking a sub part of the training data at random, computing the \"stochastic\" gradient and updating the parameters of the model.\n",
    "The algorithm is the following:\n",
    "\n",
    "1. Initialize the model's parameter $\\mathbf{w}$ and $\\mathbf{b}$ at random.\n",
    "2. Iterate for a certain number of iterations:\n",
    "    1. Select a sub part $B$ of the dataset $\\{\\mathbf{x}_i, y_i\\}$ for $i$ sample at random.\n",
    "    2. Compute the gradient of the loss with respect to these data points:\n",
    "    $$\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\widetilde L}{\\partial w_j} = \\frac1{|B|}\\sum_{i\\in B}(\\mathbf{\\hat y}_{ij} - Y_{ij})\\mathbf{x_i}\\\\\n",
    "        \\frac{\\partial \\widetilde L}{\\partial b_j} = \\frac1{|B|}\\sum_{i\\in B} (\\mathbf{\\hat y}_{ij} - Y_{ij})\n",
    "    \\end{split}\n",
    "    $$\n",
    "    3. Update the parameters $w \\leftarrow w - \\eta\\frac{\\partial \\widetilde L}{\\partial w}$ and $b \\leftarrow b - \\eta\\frac{\\partial \\widetilde L}{\\partial b}$.\n",
    "    \n",
    "The core idea is that in average over $B$, $\\mathbb E_B \\frac{\\partial \\widetilde L}{\\partial w} = \\frac{\\partial \\widetilde L}{\\partial w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercice:**\n",
    "\n",
    "Code a stochastic gradient descent using the previous function to compute the gradient of minibatch of size 128:\n",
    "\n",
    "* Select a minibatch of `batch_size` samples in the training set. You can use the function `rng.choice` for this.\n",
    "* Compute the loss and gradient.\n",
    "* Update the parameters of the model `w` and `b` with a step size `lr`.\n",
    "* Compute the validation loss as `val_loss`.\n",
    "    \n",
    "_Note that a more complete loop would make sure each training sample is visited on each epochs. Here we simplify the loop to make it easier to read._\n",
    "\n",
    "</div>\n",
    "\n",
    "Solution in `solutions/2_logreg_sgd.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pobj = []\n",
    "l_predict_proba = []\n",
    "\n",
    "\n",
    "def logger(i, w, b, loss):\n",
    "    pobj.append(loss)\n",
    "    l_predict_proba.append(\n",
    "        partial(predict_proba_logreg, w=w.copy(), b=b.copy())\n",
    "    )\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        y_pred = predict_proba_logreg(w, b, X_val).argmax(axis=1)\n",
    "        print(f\"Iterarion {i} - validation accuracy: {np.mean(y_pred == y_val)}\")\n",
    "\n",
    "# Initialize the weights of the model\n",
    "w = np.array([[3., 2], [-2., 3.], [5., 0]])\n",
    "b = np.array([.1, -.1, .3])\n",
    "\n",
    "# Parameters of the SGD\n",
    "lr = 2e-2  # learning rate\n",
    "batch_size = 16\n",
    "patience = 100\n",
    "\n",
    "# Constants for the early stopping\n",
    "best_iter = 0\n",
    "best_loss_val = 1e100\n",
    "best_params = (w.copy(), b.copy())\n",
    "\n",
    "\n",
    "n_samples, n_features = X_train.shape\n",
    "rng = np.random.RandomState(72)\n",
    "\n",
    "for it in range(5000):\n",
    "    ##############################\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    ##########################\n",
    "\n",
    "    # Logger to monitor the progress\n",
    "    # of the training.\n",
    "    logger(it, w, b, loss)\n",
    "\n",
    "    # Early stopping mechanism:\n",
    "    # - store the best loss and params\n",
    "    # - stop if no progress after patience iterations\n",
    "    if best_loss_val > val_loss:\n",
    "        best_iter = it\n",
    "        best_loss_val = val_loss\n",
    "        best_params = (w.copy(), b.copy())\n",
    "\n",
    "    if it - best_iter >= patience:\n",
    "        print(\n",
    "            \"Stopping as no progress has been made \"\n",
    "            f\"for {patience} iterations\"\n",
    "        )\n",
    "        w, b = best_params\n",
    "        break\n",
    "\n",
    "plt.plot(pobj)\n",
    "\n",
    "y_pred = predict_proba_logreg(w, b, X_val).argmax(axis=1)\n",
    "print(f\"Final validation accuracy: {np.mean(y_pred == y_val)}\")\n",
    "\n",
    "show_decision_boundary(partial(predict_proba_logreg, w=w, b=b), data=(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_animation\n",
    "create_animation(l_predict_proba, X, y, iter_step=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
